{
 "metadata": {
  "name": "",
  "signature": "sha256:e6196607dd61c04b365dcdf3394745636fe47d3eacdb8cc3057303b0c60176d3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook is intended enable testing and evaluation of various extractor methods such as:\n",
      "\n",
      "* Heuristic Extractor\n",
      "* CRF Extractor\n",
      "* boilerpipe\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle\n",
      "import os.path\n",
      "\n",
      "api_key = cPickle.load( file( os.path.expanduser( '~/mediacloud_api_key.pickle' ), 'r' ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle\n",
      "import os.path\n",
      "\n",
      "cPickle.dump( api_key, file( os.path.expanduser( '~/mediacloud_api_key.pickle' ), 'wb' ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.path.append('../../foreign_modules/python/')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "loc_key = 'f66a50230d54afaf18822808aed649f1d6ca72b08fb06d5efb6247afe9fbae52'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import mediacloud, requests, csv, sys, os, json, cPickle\n",
      "\n",
      "def get_download( downloads_id ):\n",
      "    download = requests.get('https://api.mediacloud.org/api/v2/downloads/single/'+str(downloads_id)+'?key='+api_key)\n",
      "    return download.json()[0]\n",
      "\n",
      "def extract_story( preprocessed_lines, title, description, extractor_method ):\n",
      "    extract_params = {'key':loc_key, 'preprocessed_lines':preprocessed_lines, \n",
      "                           'story_title':title, 'story_description':description, 'extractor_method': extractor_method}\n",
      "    \n",
      "    extract_result = requests.put('http://0:3000/api/v2/extractlines/extract',data=json.dumps(extract_params), \n",
      "                                headers = {'Content-type': 'application/json'})\n",
      "    \n",
      "    extract_result.raise_for_status()\n",
      "    return extract_result.json()\n",
      "\n",
      "def get_story_lines( raw_content ):\n",
      "    story_lines_params = {'key':loc_key, 'body_html':raw_content }\n",
      "    headers = {'Content-type': 'application/json'}\n",
      "    story_lines = requests.put('http://0:3000/api/v2/extractlines/story_lines',data=json.dumps(story_lines_params), \n",
      "                               params={ 'key': loc_key },headers=headers)\n",
      "    \n",
      "    story_lines.raise_for_status()\n",
      "    \n",
      "    return story_lines"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import subprocess\n",
      "import tempfile\n",
      "import codecs\n",
      "import time\n",
      "from lxml import html\n",
      "\n",
      "#download = get_download( downloads_id )\n",
      "#raw_content = download[u'raw_content']\n",
      "\n",
      "def extract_with_boilerpipe( raw_content ):\n",
      "    with tempfile.NamedTemporaryFile( suffix='.html', delete=False ) as t:\n",
      "        #print t.name\n",
      "    \n",
      "        UTF8Writer = codecs.getwriter('utf8')\n",
      "        t.file = UTF8Writer(t.file)\n",
      "        t.file.write( raw_content )\n",
      "    \n",
      "        t.close()\n",
      "        #time.sleep( 2 )\n",
      "        print \"original article tmp file \", t.name\n",
      "        \n",
      "        #input_file = '/tmp/416655019.htm'\n",
      "        input_file = t.name\n",
      "        \n",
      "        output_tmp = tempfile.NamedTemporaryFile( suffix='.html', delete=False )\n",
      "        \n",
      "        output_file = output_tmp.name\n",
      "        #output_file = '/tmp/highlighted.html'\n",
      "        print output_file\n",
      "        print subprocess.check_output(['java', '-jar',\n",
      "                               '/home/dlarochelle/dev_scratch/boilerpipe_test/out/artifacts/boilerpipe_test_jar/boilerpipe_test.jar',\n",
      "                               input_file, output_file ] )\n",
      "        f = open( output_file, 'rb' )\n",
      "        \n",
      "        annotated_file_str = f.read()\n",
      "        \n",
      "        #t.unlink( t.name )\n",
      "        output_tmp.close()\n",
      "        #output_tmp.unlink( output_tmp.name )\n",
      "        \n",
      "    return annotated_file_str\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import readability\n",
      "\n",
      "def extract_with_python_readability( raw_content ):\n",
      "    doc = readability.Document( raw_content )\n",
      "    \n",
      "    return [ doc.short_title(),\n",
      "             doc.summary() ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import goose\n",
      "\n",
      "def extract_with_python_goose( raw_content ):\n",
      "    g = goose.Goose()\n",
      "    \n",
      "    r = g.extract( raw_html=raw_content )\n",
      "    return [r.title, r.cleaned_text ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import justext\n",
      "\n",
      "def extract_with_justext( raw_content ):\n",
      "    ret = []\n",
      "    \n",
      "    paragraphs = justext.justext( raw_content, justext.get_stoplist('Portuguese') )\n",
      "    \n",
      "    #p = paragraphs[0]\n",
      "    for p in paragraphs:\n",
      "        if not p.is_boilerplate:\n",
      "            ret.append(p.text)\n",
      "            \n",
      "    return ret\n",
      "\n",
      "#extract_with_justext( raw_content )\n",
      "#raw_html\n",
      "\n",
      "#justext.get_stoplists()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import operator\n",
      "\n",
      "def get_extractor_training_text( downloads_id, preprocessed_lines ):\n",
      "    extractor_training_lines_result = requests.get(\n",
      "                                                   'https://api.mediacloud.org/api/v2/extractlines/extractor_training_lines/' + str(downloads_id),\n",
      "                                               headers = {'Content-type': 'application/json'}\n",
      "                                               , params= {'key': api_key}\n",
      "                                               )\n",
      "\n",
      "    extractor_training_lines_result.raise_for_status()\n",
      "\n",
      "    extractor_training_lines_result = extractor_training_lines_result.json()\n",
      "    \n",
      "    line_numbers = [ x['line_number'] for x in extractor_training_lines_result ]\n",
      "    \n",
      "    line_numbers = sorted(line_numbers)\n",
      "    \n",
      "    line_numbers.sort()\n",
      "    \n",
      "    #print line_numbers\n",
      "    \n",
      "    return operator.itemgetter( * line_numbers )( preprocessed_lines  )\n",
      "\n",
      "import operator\n",
      "\n",
      "def get_extracted_text( extractor_results ):\n",
      "    included_line_numbers = extractor_results['included_line_numbers']\n",
      "    #print included_line_numbers\n",
      "    \n",
      "    dl = extractor_results['download_lines']\n",
      "   \n",
      "    if len( included_line_numbers ) == 0:\n",
      "        return []\n",
      "    else:    \n",
      "        return operator.itemgetter( * extractor_results['included_line_numbers']   )(dl)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def html_strip( str ):\n",
      "    if str.isspace() or str == '':\n",
      "        return u' '\n",
      "    \n",
      "    if str == '<':\n",
      "        return u' '        \n",
      "    \n",
      "    try:\n",
      "        return html.fromstring(str).text_content()    \n",
      "    except:\n",
      "        print \"Unexpected error on string '\" + str + \"'\" , sys.exc_info()[0]\n",
      "        #raise\n",
      "        return u''\n",
      "    \n",
      "    \n",
      "\n",
      "def clean_for_comparison( str ):\n",
      "    if len(str) > 0:\n",
      "        ret = html_strip( str )\n",
      "    else:\n",
      "        return str\n",
      "    \n",
      "    if len(ret) > 0:\n",
      "        ret = ret.strip()\n",
      "    \n",
      "    return ret    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import Levenshtein\n",
      "\n",
      "def lines_to_comparable_text( lines ):\n",
      "    text = \" \".join([ clean_for_comparison(line) for line in lines ])\n",
      "    \n",
      "    if text == '':\n",
      "        text = u''\n",
      "        \n",
      "    return text\n",
      "\n",
      "def compare_accuracy( lines, lines_expected ):\n",
      "    return Levenshtein.distance( lines_to_comparable_text( lines ) , lines_to_comparable_text( lines_expected ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_anncestors( element ):\n",
      "    anncestors = [ element ];\n",
      "    anncestor = element.getparent()\n",
      "    \n",
      "    while anncestor != None :\n",
      "        #print 'loop'\n",
      "        anncestors.append( anncestor )\n",
      "        anncestor = anncestor.getparent()\n",
      "        \n",
      "    return anncestors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def text_from_lxml_object( obj):\n",
      "    if type(obj) is etree._ElementStringResult:\n",
      "        return u'' + obj\n",
      "    if type(obj) ==  etree._ElementUnicodeResult:\n",
      "        return u'' + obj \n",
      "    else:\n",
      "        try:\n",
      "            return etree.tostring( obj , method='text', encoding=\"UTF-8\") \n",
      "        except:\n",
      "            print type(obj)\n",
      "            print obj\n",
      "            \n",
      "            raise ''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from lxml import etree\n",
      "\n",
      "downloads_id =  582817308 \n",
      "download = get_download( downloads_id )\n",
      "raw_content = download[ 'raw_content' ]\n",
      "with open( '/tmp/' + str(downloads_id) , 'wb' ) as f:\n",
      "    f.write( raw_content )\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def text_children( element):\n",
      "    ret =  [ t for t in element.xpath(\"//text()\" ) if t.getparent() == element ]\n",
      "    assert len( ret ) <= 2\n",
      "    \n",
      "    if len( ret ) == 2:\n",
      "        assert ret[0].is_text\n",
      "        assert ret[1].is_tail\n",
      "    \n",
      "    for r in ret:\n",
      "        if r.is_text:\n",
      "            assert element.text == r\n",
      "        else:\n",
      "            assert r.is_tail\n",
      "            assert element.tail == r\n",
      "            \n",
      "    return ret"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#start_container[ annotation['start_offset']: annotation['end_offset'] + 1 ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_annotated_text( raw_content, annotation):\n",
      "\n",
      "    htmlparser = etree.HTMLParser()\n",
      "    root = etree.fromstring( raw_content, htmlparser )\n",
      "    \n",
      "    #print annotation[ 'start_xpath' ]\n",
      "    \n",
      "    start_container = get_element_from_xpath(root, annotation[ 'start_xpath' ] )\n",
      "    end_container   = get_element_from_xpath( root, annotation[ 'end_xpath' ] )\n",
      "    \n",
      "    if ( start_container == end_container ):\n",
      "        return [start_container[ annotation['start_offset']: annotation['end_offset'] + 1 ]]\n",
      "    \n",
      "        \n",
      "    if start_container.getparent() == end_container.getparent():\n",
      "        common_parent = start_container.getparent()\n",
      "        assert start_container.is_text\n",
      "        assert end_container.is_tail\n",
      "        assert common_parent.text == start_container\n",
      "        assert common_parent.tail == end_container\n",
      "        \n",
      "        return [start_container[ annotation['start_offset']:], end_container[ : annotation['end_offset'] + 1 ]]\n",
      "\n",
      "    \n",
      "    start_anncestors = get_anncestors( start_container )\n",
      "    end_anncestors   = get_anncestors( end_container )\n",
      "    \n",
      "    print 'sc', start_container\n",
      "    print 'ec', end_container\n",
      "    print 'common'\n",
      "    \n",
      "    middle_contents = []\n",
      "    \n",
      "    p = start_container.getparent()\n",
      "    prev_p = start_container\n",
      "    \n",
      "    if start_container.is_text:\n",
      "        # append the tail\n",
      "        texts = text_children( start_container.getparent() )\n",
      "        if len( texts ) == 2:\n",
      "            assert texts[1].is_tail\n",
      "            middle_contents.append( texts[1] )\n",
      "                                   \n",
      "    \n",
      "    while p not in end_anncestors:\n",
      "        #print \"parent:\", p, \"\\n\", etree.tostring( p )\n",
      "        assert p in start_anncestors\n",
      "        if prev_p != start_container:\n",
      "            child_index = p.index( prev_p )\n",
      "            if (child_index + 1) < len( list( p )):\n",
      "                el = list(p)[ child_index + 1]\n",
      "            else:\n",
      "                el = None\n",
      "                #print \"stripping previous parent is the last child of curparrent\"\n",
      "        else:\n",
      "            el = None\n",
      "    \n",
      "        while (el not in end_anncestors) and (el != None) :\n",
      "            #print \"inner loop\"\n",
      "            #print el\n",
      "            #print etree.tostring( el )\n",
      "            middle_contents.append( el )\n",
      "            print el\n",
      "            el = el.getnext()\n",
      "            \n",
      "        print \"end inner loop\"\n",
      "        prev_p = p\n",
      "        p = p.getparent()\n",
      "    \n",
      "    print \"end loop\"\n",
      "    \n",
      "    print p\n",
      "    commonanncestors = list([ s for s in start_anncestors if s in end_anncestors ] )\n",
      "    \n",
      "    assert p in commonanncestors\n",
      "    commonanncestor = commonanncestors[0]\n",
      "    \n",
      "    print commonanncestors\n",
      "    print start_container == end_container\n",
      "    assert p == commonanncestor\n",
      "    \n",
      "    print \"commonacccestor\", commonanncestor \n",
      "    print etree.tostring( commonanncestor )\n",
      "       \n",
      "    processed_children = [ c for c in list (commonanncestor) if c in start_anncestors ]\n",
      "    print \"ca's processed children\"\n",
      "    #print processed_children\n",
      "    #print [ etree.tostring( c ) + \"\\n\" for c in list( commonanncestor ) ] \n",
      "    assert( len( processed_children ) == 1 )\n",
      "    processed_child = processed_children[ 0]\n",
      "    \n",
      "    print \"processed_child\", processed_child\n",
      "    \n",
      "    print etree.tostring( processed_child )\n",
      "    \n",
      "    el = processed_child.getnext()\n",
      "    \n",
      "    print \"start True\"\n",
      "    \n",
      "    assert el != None\n",
      "    \n",
      "    #print etree.tostring( el )\n",
      "    \n",
      "    while True:\n",
      "        \n",
      "            print 'outer loop:',  el, \"\\n\", etree.tostring( el )\n",
      "\n",
      "\n",
      "\n",
      "            while (el not in end_anncestors):\n",
      "                print \"inner loop\"\n",
      "                print el\n",
      "                assert el != None\n",
      "                print etree.tostring( el )\n",
      "                middle_contents.append( el )\n",
      "                el = el.getnext()\n",
      "                assert el != None\n",
      "                \n",
      "            print 'end inner loop'\n",
      "            print el\n",
      "            print etree.tostring( el )\n",
      "            # element is an ancester of end_container and has no (non-text)children\n",
      "            # since end_container is text el must be it's parent so we can stop\n",
      "            if (len(list(el)) == 0 ):\n",
      "                            \n",
      "                print 'found end_contain parent, exiting loop:',  el, \"\\n\", etree.tostring( el )\n",
      "                \n",
      "                assert end_container in text_children(el)\n",
      "    \n",
      "                assert end_container.getparent() == el\n",
      "                break\n",
      "            \n",
      "            ## HACK bc/ lxml/etree doesn't have a real text node\n",
      "            ## treat the text as the first child node\n",
      "            texts = text_children(el)\n",
      "            \n",
      "            assert ( all ( [t.getparent() == el for t in texts] ) )\n",
      "            \n",
      "            assert( len (texts ) <= 2 )\n",
      "            if len( texts) > 0:\n",
      "                if texts[0].is_text:\n",
      "                    assert el.text == texts[0]\n",
      "                    middle_contents.append(texts[0] );\n",
      "    \n",
      "            el = el[0]\n",
      "        \n",
      "            assert el != None\n",
      "        \n",
      "            \n",
      "    #[ text_from_lxml_object( mc ) for mc in middle_contents ]\n",
      "    print etree.tostring(el)\n",
      "    #print etree.tostring(middle_contents[-1] )\n",
      "    print end_container.is_tail\n",
      "    print end_container.is_text\n",
      "    print \"escape while\"\n",
      "    #print list ( el.itertext() )\n",
      "    #print 'ca'\n",
      "    #commonanncestor.text\n",
      "    #type(el)\n",
      "    type( end_container )\n",
      "    assert end_container in text_children(el)\n",
      "    \n",
      "    assert el == end_container.getparent()\n",
      "    \n",
      "    print etree.tostring( end_container.getparent() )\n",
      "    #print middle_contents[-2:] \n",
      "    #print end_container\n",
      "    text_children(el )\n",
      "    etree.tostring( el.getparent() )\n",
      "    #p_el = el.getparent()\n",
      "    #print etree.tostring( p_el )\n",
      "    #text_children( p_el )[0].is_text\n",
      "    \n",
      "    #annotation\n",
      "    #list(p_el)\n",
      "    if end_container.is_tail:\n",
      "        assert len(text_children(el )) == 2\n",
      "        \n",
      "        middle_contents.append( text_children( el )[ 0] )\n",
      "    \n",
      "    else:\n",
      "        assert end_container.is_text\n",
      "        \n",
      "    end_text = end_container[:annotation['end_offset'] - 1]\n",
      "    \n",
      "    print 'start container'\n",
      "    print etree.tostring( start_container.getparent())\n",
      "    print start_container\n",
      "    print 'offset', annotation['start_offset']\n",
      "    \n",
      "    print 'end container'\n",
      "    print etree.tostring( end_container.getparent())\n",
      "    print end_container\n",
      "    print 'offset', annotation['end_offset']\n",
      "    \n",
      "    #assert start_container.is_text\n",
      "    \n",
      "    start_text = start_container[annotation['start_offset']:]\n",
      "    \n",
      "    target_text = [ start_text ]\n",
      "    target_text.extend( [ text_from_lxml_object( mc ) for mc in middle_contents ] )\n",
      "    target_text.append( end_text )\n",
      "    \n",
      "    return target_text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import difflib\n",
      "from IPython.display import HTML\n",
      "\n",
      "from collections import Counter\n",
      "\n",
      "def ratcliff_obershelp_compare( actual_lines, expected_lines ):\n",
      "    \n",
      "    words_expected = lines_to_comparable_text(expected_lines ).split()\n",
      "    words_crf      = lines_to_comparable_text(actual_lines ).split()\n",
      "    \n",
      "    differ = difflib.Differ( )\n",
      "    \n",
      "    #print words_crf[:10]\n",
      "    #print words_expected[:10]\n",
      "    list( differ.compare( words_crf , words_expected ) )\n",
      "    counts = Counter([ d[0] for d in differ.compare( words_expected, words_crf   ) ])\n",
      "    \n",
      "    tp = counts[' ']\n",
      "    fp = counts['+']\n",
      "    fn = counts['-']\n",
      "    \n",
      "    if float(tp+fp) == 0:\n",
      "        precision = 0.0\n",
      "    else:\n",
      "        precision = tp/float(tp+fp)\n",
      "        \n",
      "    if float( tp + fn ) == 0:\n",
      "        recall = 0\n",
      "    else:\n",
      "        recall    = tp/float( tp + fn )\n",
      "    \n",
      "    if ( precision + recall ) > 0:\n",
      "        f1 = 2*(precision*recall)/( precision + recall )\n",
      "    else:\n",
      "        f1 = 0\n",
      "    \n",
      "    ret = { 'precision': precision,\n",
      "        'recall': recall,\n",
      "        'f1': f1\n",
      "    }\n",
      "    \n",
      "    return ret\n",
      "\n",
      "#ratcliff_obershelp_compare( words_crf, words_expected )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#downloads_id\n",
      "#story\n",
      "#raw_content\n",
      "#expected_lines\n",
      "#preprocessed_lines\n",
      "\n",
      "def create_extractor_training_object( downloads_id, expected_lines=None ):\n",
      "    download = get_download( downloads_id )\n",
      "    \n",
      "    raw_content = download[u'raw_content']\n",
      "    stories_id = download[u'stories_id']\n",
      "    \n",
      "    print download['url']\n",
      "    \n",
      "    story = requests.get('https://api.mediacloud.org/api/v2/stories/single/'+str(stories_id)+'?key='+api_key)\n",
      "    \n",
      "    story = story.json()[0]\n",
      "    \n",
      "    story_lines = get_story_lines( raw_content )\n",
      "    #print story_lines.content\n",
      "    preprocessed_lines = story_lines.json()\n",
      "    \n",
      "    if not expected_lines:\n",
      "        expected_lines = get_extractor_training_text( downloads_id, preprocessed_lines  )\n",
      "\n",
      "    ret = { 'downloads_id': downloads_id,\n",
      "           'raw_content': raw_content,\n",
      "           'media_id': story['media_id'],\n",
      "           'story': story,\n",
      "           'preprocessed_lines': preprocessed_lines,\n",
      "           'expected_lines': expected_lines\n",
      "           }\n",
      "    \n",
      "    return ret"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compare_extractors_for_download( downloads_id ):\n",
      "    \n",
      "    eto = create_extractor_training_object( downloads_id )\n",
      "    \n",
      "    return comp_extractors( eto )\n",
      "\n",
      "    download = get_download( downloads_id )\n",
      "    \n",
      "    raw_content = download[u'raw_content']\n",
      "    stories_id = download[u'stories_id']\n",
      "     \n",
      "    story = requests.get('https://api.mediacloud.org/api/v2/stories/single/'+str(stories_id)+'?key='+api_key)\n",
      "    \n",
      "    story = story.json()[0]\n",
      "    \n",
      "    story_lines = get_story_lines( raw_content )\n",
      "    #print story_lines.content\n",
      "    preprocessed_lines = story_lines.json()\n",
      "    \n",
      "    expected_lines = get_extractor_training_text( downloads_id, preprocessed_lines  )\n",
      "    \n",
      "    \n",
      "    \n",
      "def comp_extractors( eto ):    \n",
      "    downloads_id = eto['downloads_id']\n",
      "    story = eto['story']\n",
      "    raw_content = eto['raw_content']\n",
      "    preprocessed_lines = eto['preprocessed_lines']\n",
      "    expected_lines = eto['expected_lines']\n",
      "        \n",
      "    title = story[u'title']\n",
      "    description = story[u'description']\n",
      "    url = story[u'url']\n",
      "    \n",
      "    heur_extract = extract_story( preprocessed_lines, title, description, 'HeuristicExtractor')\n",
      "    crf_extract = extract_story( preprocessed_lines, title, description, 'CrfExtractor')\n",
      "    \n",
      "    heur_lines = get_extracted_text( heur_extract )\n",
      "    crf_lines  = get_extracted_text( crf_extract )\n",
      "    \n",
      "    python_readability_lines = extract_with_python_readability( raw_content )\n",
      "    \n",
      "    py_goose_lines = extract_with_python_goose( raw_content )\n",
      "    justext_lines  = extract_with_justext( raw_content )\n",
      "    global glob_expected_lines\n",
      "    global glob_crf_lines\n",
      "    glob_expected_lines = expected_lines\n",
      "    glob_crf_lines      = crf_lines\n",
      "    \n",
      "    #tree = html.fromstring( extract_with_boilerpipe( raw_content) )   \n",
      "    #spans = tree.xpath('//span[@class=\"x-boilerpipe-mark1\"]')\n",
      "    #boiler_pipe_lines = [ s.text for s in spans ]\n",
      "\n",
      "    #print \"expected_lines:\"\n",
      "    #print lines_to_comparable_text(expected_lines)\n",
      "    \n",
      "    #print \"boilerpipe lines\"\n",
      "    #print lines_to_comparable_text(boiler_pipe_lines)\n",
      "    \n",
      "    comp_results = {}\n",
      "    comp_results['heur'] = ratcliff_obershelp_compare( heur_lines, expected_lines )\n",
      "    comp_results['crf']  = ratcliff_obershelp_compare( crf_lines, expected_lines )\n",
      "    #comp_results['boiler_pipe'] = ratcliff_obershelp_compare( boiler_pipe_lines, expected_lines )\n",
      "    comp_results['python_readibilty'] = ratcliff_obershelp_compare( python_readability_lines, expected_lines )\n",
      "    comp_results['py_goose'] = ratcliff_obershelp_compare( py_goose_lines, expected_lines )\n",
      "    comp_results['justext']  = ratcliff_obershelp_compare( justext_lines, expected_lines )\n",
      "    \n",
      "    comp_results['downloads_id'] = downloads_id\n",
      "    \n",
      "    #comp_results['expected']    = compare_accuracy( expected_lines, expected_lines )\n",
      "    return comp_results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#comps_expected = comps"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "downloads_id = 416655019\n",
      "downloads_ids = [391881020,401370599,412896439,412952145,412977048,413024519,413657081,413835576,414040102,414257623,\n",
      "                 414377428,414480464,414818749,414983458,415185946,415186582,415197547,415424551,415978069,416026460,\n",
      "                 416026587,416047494,416047513,416210404,416263840,416306952,416426245,416655019,416730837,416802690,\n",
      "                 417347290,417347524,417368539,417389613,417477837,417653177,418489742,418544762,418574641,418648698,\n",
      "                 418661859,419404469,419440474,419483895,419873979,420430754,420599387,420666122,421520860,421834553,\n",
      "                 422181106,422280595,422910963,423318170,424080271,424369085,424796346,424840366,425206279,426405203,\n",
      "                 426560018,426632784,426709900,428449440,429607289,430363249,430995428,433457459,435624796,435659593,461175103,461175549,461176415,461176844,461177487,461178557,461178590,461179203,461179222,461179441,461179762,461179818,461179954,461179956,461180307,461181039,461181597,461186137,461186258,461186833,461187188,461187261,461187577,461188549,461189069,461190586,461193383]\n",
      "\n",
      "print len( downloads_ids )\n",
      "\n",
      "comps = []\n",
      "\n",
      "extractor_training_objects = []\n",
      "for downloads_id in downloads_ids[:10]:\n",
      "    print 'downloads_id:', downloads_id\n",
      "    extractor_training_objects.append( create_extractor_training_object( downloads_id ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "97\n",
        "downloads_id: 391881020\n",
        "http://virgula.uol.com.br/inacreditavel/bizarro/metro-tem-confusao-por-causa-de-encontro-em-shopping"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "downloads_id:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 401370599\n",
        "http://www.politicalivre.com.br/2013/12/policia-frustra-manifestacao-em-shopping-na-zona-sul-de-sao-paulo/"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "downloads_id:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 412896439\n",
        "http://ultimainstancia.uol.com.br/conteudo/noticias/68515/dez+jovens+serao+intimados+por+%27rolezinho%27.shtml"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "downloads_id:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 412952145\n",
        "http://coturnonoturno.blogspot.com/2014/01/um-rolezinho-pelos-numeros-do.html"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "downloads_id:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 412977048\n",
        "http://www.bemparana.com.br/noticia/298544/tres-mil-confirmam-rolezinho-no-shopping-leblon"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "downloads_id:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 413024519\n",
        "http://estadao.feedsportal.com/c/33043/f/534105/s/35d9a7c4/sc/36/l/0L0Sestadao0N0Bbr0Cnoticias0Ccidades0Hguarda0Ecivil0Ee0Eferido0Edurante0Erolezinho0Eem0Eparque0Eda0Egrande0Esao0Epaulo0H11179710H0A0Bhtm/story01.htm"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "downloads_id:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 413657081\n",
        "http://redir.folha.com.br/redir/online/emcimadahora/rss091/*http://www1.folha.uol.com.br/cotidiano/2014/01/1397453-associacao-dos-shoppings-fara-reuniao-de-emergencia-sobre-rolezinhos.shtml"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "downloads_id:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 413835576\n",
        "http://www.conversaafiada.com.br/brasil/2014/01/14/tijolaco-elege-a-madrinha-do-role-a-tucanhede/"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "downloads_id:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 414040102\n",
        "http://www.otempo.com.br/jovens-marcam-novo-rolezinho-1.773450"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "downloads_id:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 414257623\n",
        "http://www.ovale.com.br/ultimas/shoppings-sp-agora-sem-teto-prometem-rolez-o-na-zona-sul-1.488662?localLinksEnabled=false"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for extractor_training_object in extractor_training_objects:\n",
      "    res = comp_extractors( extractor_training_object )\n",
      "    #print res\n",
      "    comps.append( res )\n",
      "    \n",
      "\n",
      "#print 'Comps_expteced', comps_expected\n",
      "#print 'comps', comps\n",
      "#comps == comps_expected"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "extractor_training_objects = []"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sqlite3\n",
      "\n",
      "db = sqlite3.connect('extractor_train_dbs/dev_2014-11-03T16_24_46-0500.db')\n",
      "db.row_factory = sqlite3.Row\n",
      "\n",
      "cursor = db.cursor()\n",
      "\n",
      "cursor.execute( \"SELECT * from dlannotations  where selected_texts_json is not null order by downloads_id\" )\n",
      "\n",
      "extractor_training_objects = []\n",
      "\n",
      "skipped_downloads = 0\n",
      "for row in list( cursor.fetchall() )[:10]:\n",
      "    row =  dict([ (k, row[k]) for k in row.keys() ])\n",
      "    \n",
      "    #print row\n",
      "    \n",
      "    row['annotations'] = json.loads( row['annotations_json'] )\n",
      "    row['raw_content'] = u'' + row['raw_content']\n",
      "    row['selected_texts'] = json.loads( row['selected_texts_json'] )\n",
      "    annotated_text  = []\n",
      "    \n",
      "    try:\n",
      "        annotations = row['annotations']\n",
      "        download = get_download( row['downloads_id'] )\n",
      "    \n",
      "        assert row['selected_texts'] != None\n",
      "        assert row['selected_texts'] > 0\n",
      "        \n",
      "        eto = create_extractor_training_object( row['downloads_id'], expected_lines=row['selected_texts'] )\n",
      "        \n",
      "        #assert eto['raw_content'] != row['raw_content']\n",
      "        \n",
      "        if eto['raw_content'] != row['raw_content']:\n",
      "            #TODO figure out why these may differ\n",
      "            pass\n",
      "        \n",
      "            #d = difflib.Differ()\n",
      "            #diff = d.compare(eto['raw_content'].splitlines(1), row['raw_content'].splitlines(1))\n",
      "            #print '\\n'.join(diff)\n",
      "        \n",
      "        extractor_training_objects.append( eto )\n",
      "        \n",
      "    except:\n",
      "        print \"error\"\n",
      "        print 'downloads_id', row['downloads_id']\n",
      "        #print annotation\n",
      "        skipped_downloads += 1\n",
      "        #raise\n",
      "\n",
      "print \"skipped\", skipped_downloads\n",
      "print \"processed\", len(extractor_training_objects)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "http://www.gazeta.ru/travel/news/2014/01/10/n_5866529.shtml\n",
        "http://www.elwatannews.com/news/details/393669"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://www.gazeta.ru/business/news/2014/01/23/n_5897125.shtml"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://www.arabnet5.com/news.asp?c=2&id=234426"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://www.novayagazeta.ru/society/62102.html"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://www.svoboda.org/content/article/25259442.html"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://echo.msk.ru/blog/avakov/1256990-echo/"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://lenta.ru/news/2014/02/14/ballers/"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://www.novayagazeta.ru/news/318845.html"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://arabic.rt.com/news/653679/"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipped"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0\n",
        "processed 10\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len( extractor_training_objects )\n",
      "\n",
      "comps_new_downloads = []\n",
      "for extractor_training_object in extractor_training_objects:\n",
      "    res = comp_extractors( extractor_training_object )\n",
      "    #print res\n",
      "    comps_new_downloads.append( res )\n",
      "\n",
      "#extractor_training_objects\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "\n",
      "\n",
      "new_comps = []\n",
      "for comp in comps_new_downloads:\n",
      "    new_comp = {}\n",
      "    new_comp = { 'downloads_id': comp['downloads_id'] }\n",
      "    #del comp['downloads_id']\n",
      "    \n",
      "    extractor_types = [ k for k in comp.keys() if k != 'downloads_id' ]\n",
      "    \n",
      "    for extractor_type in extractor_types:\n",
      "        new_comp.update([ ( k + '_' + extractor_type , v) for k,v in comp[ extractor_type ].iteritems() ])\n",
      "        #new_comp[ k + 'boiler_pipe      \n",
      "\n",
      "    new_comps.append( new_comp )\n",
      "    \n",
      "new_comps\n",
      "df = pd.DataFrame( new_comps )\n",
      "df.set_index('downloads_id', inplace=True )\n",
      "df.describe(percentiles=[.5] )\n",
      "result_types = [ 'precision', 'recall', 'f1' ]\n",
      "for result_type in result_types:\n",
      "    res_columns = [ col for col in df.columns if col.startswith( result_type ) ]\n",
      "    #df.ix[:,['f1_boiler_pipe',\t'f1_crf',\t'f1_heur', 'f1_python_readibilty']].describe()\n",
      "    print df.ix[:,res_columns].describe( percentiles=[0.5])\n",
      "#df.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "       precision_crf  precision_heur  precision_justext  precision_py_goose  \\\n",
        "count      10.000000       10.000000                 10           10.000000   \n",
        "mean        0.910943        0.692536                  0            0.857619   \n",
        "std         0.097287        0.295361                  0            0.173451   \n",
        "min         0.700000        0.114883                  0            0.571429   \n",
        "50%         0.938708        0.725128                  0            0.928571   \n",
        "max         1.000000        1.000000                  0            1.000000   \n",
        "\n",
        "       precision_python_readibilty  \n",
        "count                    10.000000  \n",
        "mean                      0.967210  \n",
        "std                       0.043097  \n",
        "min                       0.867925  \n",
        "50%                       0.982189  \n",
        "max                       1.000000  \n",
        "       recall_crf  recall_heur  recall_justext  recall_py_goose  \\\n",
        "count   10.000000    10.000000              10        10.000000   \n",
        "mean     0.661963     0.928800               0         0.050477   \n",
        "std      0.416339     0.076623               0         0.038965   \n",
        "min      0.012588     0.733668               0         0.002014   \n",
        "50%      0.862237     0.952980               0         0.047638   \n",
        "max      1.000000     0.988296               0         0.130435   \n",
        "\n",
        "       recall_python_readibilty  \n",
        "count                 10.000000  \n",
        "mean                   0.936305  \n",
        "std                    0.086049  \n",
        "min                    0.733668  \n",
        "50%                    0.983594  \n",
        "max                    1.000000  \n",
        "          f1_crf    f1_heur  f1_justext  f1_py_goose  f1_python_readibilty\n",
        "count  10.000000  10.000000          10    10.000000             10.000000\n",
        "mean    0.675824   0.756309           0     0.092885              0.948940\n",
        "std     0.378868   0.246934           0     0.068436              0.050343\n",
        "min     0.024863   0.205128           0     0.004020              0.831909\n",
        "50%     0.888183   0.789205           0     0.089764              0.952653\n",
        "max     0.980716   0.991957           0     0.226415              0.997392\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>f1_crf</th>\n",
        "      <th>f1_heur</th>\n",
        "      <th>f1_justext</th>\n",
        "      <th>f1_py_goose</th>\n",
        "      <th>f1_python_readibilty</th>\n",
        "      <th>precision_crf</th>\n",
        "      <th>precision_heur</th>\n",
        "      <th>precision_justext</th>\n",
        "      <th>precision_py_goose</th>\n",
        "      <th>precision_python_readibilty</th>\n",
        "      <th>recall_crf</th>\n",
        "      <th>recall_heur</th>\n",
        "      <th>recall_justext</th>\n",
        "      <th>recall_py_goose</th>\n",
        "      <th>recall_python_readibilty</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 10.000000</td>\n",
        "      <td> 10.000000</td>\n",
        "      <td> 10</td>\n",
        "      <td> 10.000000</td>\n",
        "      <td> 10.000000</td>\n",
        "      <td> 10.000000</td>\n",
        "      <td> 10.000000</td>\n",
        "      <td> 10</td>\n",
        "      <td> 10.000000</td>\n",
        "      <td> 10.000000</td>\n",
        "      <td> 10.000000</td>\n",
        "      <td> 10.000000</td>\n",
        "      <td> 10</td>\n",
        "      <td> 10.000000</td>\n",
        "      <td> 10.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td>  0.675824</td>\n",
        "      <td>  0.756309</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0.092885</td>\n",
        "      <td>  0.948940</td>\n",
        "      <td>  0.910943</td>\n",
        "      <td>  0.692536</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0.857619</td>\n",
        "      <td>  0.967210</td>\n",
        "      <td>  0.661963</td>\n",
        "      <td>  0.928800</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0.050477</td>\n",
        "      <td>  0.936305</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td>  0.378868</td>\n",
        "      <td>  0.246934</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0.068436</td>\n",
        "      <td>  0.050343</td>\n",
        "      <td>  0.097287</td>\n",
        "      <td>  0.295361</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0.173451</td>\n",
        "      <td>  0.043097</td>\n",
        "      <td>  0.416339</td>\n",
        "      <td>  0.076623</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0.038965</td>\n",
        "      <td>  0.086049</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>  0.024863</td>\n",
        "      <td>  0.205128</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0.004020</td>\n",
        "      <td>  0.831909</td>\n",
        "      <td>  0.700000</td>\n",
        "      <td>  0.114883</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0.571429</td>\n",
        "      <td>  0.867925</td>\n",
        "      <td>  0.012588</td>\n",
        "      <td>  0.733668</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0.002014</td>\n",
        "      <td>  0.733668</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>  0.397189</td>\n",
        "      <td>  0.642529</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0.047207</td>\n",
        "      <td>  0.928200</td>\n",
        "      <td>  0.870330</td>\n",
        "      <td>  0.508367</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0.744048</td>\n",
        "      <td>  0.958217</td>\n",
        "      <td>  0.285294</td>\n",
        "      <td>  0.920114</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0.024384</td>\n",
        "      <td>  0.905114</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td>  0.888183</td>\n",
        "      <td>  0.789205</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0.089764</td>\n",
        "      <td>  0.952653</td>\n",
        "      <td>  0.938708</td>\n",
        "      <td>  0.725128</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0.928571</td>\n",
        "      <td>  0.982189</td>\n",
        "      <td>  0.862237</td>\n",
        "      <td>  0.952980</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0.047638</td>\n",
        "      <td>  0.983594</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td>  0.918237</td>\n",
        "      <td>  0.959505</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0.133739</td>\n",
        "      <td>  0.990716</td>\n",
        "      <td>  0.990964</td>\n",
        "      <td>  0.949468</td>\n",
        "      <td>  0</td>\n",
        "      <td>  1.000000</td>\n",
        "      <td>  0.999363</td>\n",
        "      <td>  0.982597</td>\n",
        "      <td>  0.976902</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0.071897</td>\n",
        "      <td>  0.992448</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td>  0.980716</td>\n",
        "      <td>  0.991957</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0.226415</td>\n",
        "      <td>  0.997392</td>\n",
        "      <td>  1.000000</td>\n",
        "      <td>  1.000000</td>\n",
        "      <td>  0</td>\n",
        "      <td>  1.000000</td>\n",
        "      <td>  1.000000</td>\n",
        "      <td>  1.000000</td>\n",
        "      <td>  0.988296</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0.130435</td>\n",
        "      <td>  1.000000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "          f1_crf    f1_heur  f1_justext  f1_py_goose  f1_python_readibilty  \\\n",
        "count  10.000000  10.000000          10    10.000000             10.000000   \n",
        "mean    0.675824   0.756309           0     0.092885              0.948940   \n",
        "std     0.378868   0.246934           0     0.068436              0.050343   \n",
        "min     0.024863   0.205128           0     0.004020              0.831909   \n",
        "25%     0.397189   0.642529           0     0.047207              0.928200   \n",
        "50%     0.888183   0.789205           0     0.089764              0.952653   \n",
        "75%     0.918237   0.959505           0     0.133739              0.990716   \n",
        "max     0.980716   0.991957           0     0.226415              0.997392   \n",
        "\n",
        "       precision_crf  precision_heur  precision_justext  precision_py_goose  \\\n",
        "count      10.000000       10.000000                 10           10.000000   \n",
        "mean        0.910943        0.692536                  0            0.857619   \n",
        "std         0.097287        0.295361                  0            0.173451   \n",
        "min         0.700000        0.114883                  0            0.571429   \n",
        "25%         0.870330        0.508367                  0            0.744048   \n",
        "50%         0.938708        0.725128                  0            0.928571   \n",
        "75%         0.990964        0.949468                  0            1.000000   \n",
        "max         1.000000        1.000000                  0            1.000000   \n",
        "\n",
        "       precision_python_readibilty  recall_crf  recall_heur  recall_justext  \\\n",
        "count                    10.000000   10.000000    10.000000              10   \n",
        "mean                      0.967210    0.661963     0.928800               0   \n",
        "std                       0.043097    0.416339     0.076623               0   \n",
        "min                       0.867925    0.012588     0.733668               0   \n",
        "25%                       0.958217    0.285294     0.920114               0   \n",
        "50%                       0.982189    0.862237     0.952980               0   \n",
        "75%                       0.999363    0.982597     0.976902               0   \n",
        "max                       1.000000    1.000000     0.988296               0   \n",
        "\n",
        "       recall_py_goose  recall_python_readibilty  \n",
        "count        10.000000                 10.000000  \n",
        "mean          0.050477                  0.936305  \n",
        "std           0.038965                  0.086049  \n",
        "min           0.002014                  0.733668  \n",
        "25%           0.024384                  0.905114  \n",
        "50%           0.047638                  0.983594  \n",
        "75%           0.071897                  0.992448  \n",
        "max           0.130435                  1.000000  "
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = compare_extractors_for_download( 461179954 )\n",
      "res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "http://leonardoboff.wordpress.com/2014/01/23/os-rolezinhos-nos-acusam-somos-uma-sociedade-injusta-e-segregacionista/\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "{'crf': {'f1': 0.3154066985645933,\n",
        "  'precision': 0.1876993166287016,\n",
        "  'recall': 0.9868263473053892},\n",
        " 'downloads_id': 461179954,\n",
        " 'heur': {'f1': 0.9933694996986137,\n",
        "  'precision': 1.0,\n",
        "  'recall': 0.9868263473053892},\n",
        " 'justext': {'f1': 0.05956775742804918,\n",
        "  'precision': 0.030702726668144536,\n",
        "  'recall': 0.9952095808383233},\n",
        " 'py_goose': {'f1': 0.9951865222623345,\n",
        "  'precision': 1.0,\n",
        "  'recall': 0.9904191616766467},\n",
        " 'python_readibilty': {'f1': 0.2412060301507538,\n",
        "  'precision': 0.3010752688172043,\n",
        "  'recall': 0.20119760479041915}}"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import mediacloud, requests, csv, sys, os, json, cPickle\n",
      "from pyroc import *\n",
      "\n",
      "#extractor_training_lines_checked has the training lines for downloads for which the highest line listed as 'included' was less than the number of lines in the download (max(included_line_numbers) < len(story_lines.json()))\n",
      "f = open(\"extractor_training_lines_checked.json\").read()\n",
      "reader = json.loads(f)\n",
      "heur = []\n",
      "cPickle.dump(heur, open(\"heur.p\", \"wb\"))\n",
      "crf = []\n",
      "cPickle.dump(crf, open(\"crf.p\", \"wb\"))\n",
      "done = []\n",
      "cPickle.dump(done, open(\"done.p\", \"wb\"))\n",
      "\n",
      "for row in reader[:30]:\n",
      "    \n",
      "    did = row[u'downloads_id']\n",
      "    lns = row[u'included_line_numbers']\n",
      "    \n",
      "    curh, curc = gen_data(did, lns)\n",
      "    \n",
      "    heur+=curh\n",
      "    crf+=curc\n",
      "    done.append(did)\n",
      "    \n",
      "cPickle.dump(done, open(\"done.p\", \"wb\"))\n",
      "cPickle.dump(heur, open(\"heur.p\", \"wb\"))\n",
      "cPickle.dump(crf, open(\"crf.p\", \"wb\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    }
   ],
   "metadata": {}
  }
 ]
}