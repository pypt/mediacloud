{
 "metadata": {
  "name": "",
  "signature": "sha256:385a693482374c03d1400f9b6b7922a2f7c702222cfe76e6c393fbfca777bdbf"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Extractor evaluation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook evaluates both Media Cloud's internal extractors and third party FLOSS extractor libraries across a corpus of hand annotated articles.\n",
      "\n",
      "Readers may wish to skip to the results section at the end."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Set up / Methods"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle\n",
      "import os.path\n",
      "\n",
      "api_key = cPickle.load( file( os.path.expanduser( '~/mediacloud_api_key.pickle' ), 'r' ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle\n",
      "import os.path\n",
      "\n",
      "cPickle.dump( api_key, file( os.path.expanduser( '~/mediacloud_api_key.pickle' ), 'wb' ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.path.append('../../foreign_modules/python/')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "loc_key = 'f66a50230d54afaf18822808aed649f1d6ca72b08fb06d5efb6247afe9fbae52'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import mediacloud, requests, csv, sys, os, json, cPickle\n",
      "\n",
      "def get_download( downloads_id ):\n",
      "    download = requests.get('https://api.mediacloud.org/api/v2/downloads/single/'+str(downloads_id)+'?key='+api_key)\n",
      "    return download.json()[0]\n",
      "\n",
      "def extract_story( preprocessed_lines, title, description, extractor_method ):\n",
      "    extract_params = {'key':loc_key, 'preprocessed_lines':preprocessed_lines, \n",
      "                           'story_title':title, 'story_description':description, 'extractor_method': extractor_method}\n",
      "    \n",
      "    extract_result = requests.put('http://0:3000/api/v2/extractlines/extract',data=json.dumps(extract_params), \n",
      "                                headers = {'Content-type': 'application/json'})\n",
      "    \n",
      "    extract_result.raise_for_status()\n",
      "    return extract_result.json()\n",
      "\n",
      "def get_story_lines( raw_content ):\n",
      "    story_lines_params = {'key':loc_key, 'body_html':raw_content }\n",
      "    headers = {'Content-type': 'application/json'}\n",
      "    story_lines = requests.put('http://0:3000/api/v2/extractlines/story_lines',data=json.dumps(story_lines_params), \n",
      "                               params={ 'key': loc_key },headers=headers)\n",
      "    \n",
      "    story_lines.raise_for_status()\n",
      "    \n",
      "    return story_lines"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import subprocess\n",
      "import tempfile\n",
      "import codecs\n",
      "import time\n",
      "from lxml import html\n",
      "\n",
      "#download = get_download( downloads_id )\n",
      "#raw_content = download[u'raw_content']\n",
      "\n",
      "def extract_with_boilerpipe( raw_content ):\n",
      "    with tempfile.NamedTemporaryFile( suffix='.html', delete=False ) as t:\n",
      "        #print t.name\n",
      "    \n",
      "        UTF8Writer = codecs.getwriter('utf8')\n",
      "        t.file = UTF8Writer(t.file)\n",
      "        t.file.write( raw_content )\n",
      "    \n",
      "        t.close()\n",
      "        #time.sleep( 2 )\n",
      "        #print \"original article tmp file \", t.name\n",
      "        \n",
      "        #input_file = '/tmp/416655019.htm'\n",
      "        input_file = t.name\n",
      "        \n",
      "        output_tmp = tempfile.NamedTemporaryFile( suffix='.html', delete=False )\n",
      "        \n",
      "        output_file = output_tmp.name\n",
      "        #output_file = '/tmp/highlighted.html'\n",
      "        #print output_file\n",
      "        \n",
      "        subprocess.check_output(['java', '-jar',\n",
      "                               '/home/dlarochelle/dev_scratch/boilerpipe_test/out/artifacts/boilerpipe_test_jar/boilerpipe_test.jar',\n",
      "                               input_file, output_file ] )\n",
      "        f = open( output_file, 'rb' )\n",
      "        \n",
      "        annotated_file_str = f.read()\n",
      "        \n",
      "        #t.unlink( t.name )\n",
      "        output_tmp.close()\n",
      "        #output_tmp.unlink( output_tmp.name )\n",
      "\n",
      "    tree = html.fromstring( annotated_file_str )   \n",
      "    spans = tree.xpath('//span[@class=\"x-boilerpipe-mark1\"]')\n",
      "    boiler_pipe_lines = [ etree.tostring(s) for s in spans ]\n",
      "\n",
      "    ret = { 'extracted_html': boiler_pipe_lines }\n",
      "    return ret\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#f = open( '/tmp/tmp01CV6F.html' )\n",
      "#annotated_file_str = f.read()\n",
      "#tree = html.fromstring( annotated_file_str )   \n",
      "#spans = tree.xpath('//span[@class=\"x-boilerpipe-mark1\"]')\n",
      "#span = spans[0]\n",
      "#etree.tostring( span )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import readability\n",
      "\n",
      "def extract_with_python_readability( raw_content ):\n",
      "    doc = readability.Document( raw_content )\n",
      "    \n",
      "    return [ doc.short_title(),\n",
      "             doc.summary() ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import goose\n",
      "\n",
      "def extract_with_python_goose( raw_content ):\n",
      "    g = goose.Goose()\n",
      "    \n",
      "    r = g.extract( raw_html=raw_content )\n",
      "    return [r.title, r.cleaned_text ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import justext\n",
      "\n",
      "def extract_with_justext( raw_content ):\n",
      "    ret = []\n",
      "    \n",
      "    paragraphs = justext.justext( raw_content, justext.get_stoplist('English') )\n",
      "    \n",
      "    #p = paragraphs[0]\n",
      "    for p in paragraphs:\n",
      "        if not p.is_boilerplate:\n",
      "            ret.append(p.text)\n",
      "            \n",
      "    return ret\n",
      "\n",
      "#extract_with_justext( raw_content )\n",
      "#raw_html\n",
      "\n",
      "#justext.get_stoplists()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import operator\n",
      "\n",
      "def get_extractor_training_text( downloads_id, preprocessed_lines ):\n",
      "    extractor_training_lines_result = requests.get(\n",
      "                                                   'https://api.mediacloud.org/api/v2/extractlines/extractor_training_lines/' + str(downloads_id),\n",
      "                                               headers = {'Content-type': 'application/json'}\n",
      "                                               , params= {'key': api_key}\n",
      "                                               )\n",
      "\n",
      "    extractor_training_lines_result.raise_for_status()\n",
      "\n",
      "    extractor_training_lines_result = extractor_training_lines_result.json()\n",
      "    \n",
      "    line_numbers = [ x['line_number'] for x in extractor_training_lines_result ]\n",
      "    \n",
      "    line_numbers = sorted(line_numbers)\n",
      "    \n",
      "    line_numbers.sort()\n",
      "    \n",
      "    #print line_numbers\n",
      "    \n",
      "    return operator.itemgetter( * line_numbers )( preprocessed_lines  )\n",
      "\n",
      "import operator\n",
      "\n",
      "def get_extracted_text( extractor_results ):\n",
      "    included_line_numbers = extractor_results['included_line_numbers']\n",
      "    #print included_line_numbers\n",
      "    \n",
      "    dl = extractor_results['download_lines']\n",
      "   \n",
      "    if len( included_line_numbers ) == 0:\n",
      "        return []\n",
      "    else:    \n",
      "        return operator.itemgetter( * extractor_results['included_line_numbers']   )(dl)\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import Levenshtein\n",
      "\n",
      "def lines_to_comparable_text( lines ):\n",
      "    text = u\"\\n\\n\".join([ clean_for_comparison(line) for line in lines ])\n",
      "    \n",
      "    if text == '':\n",
      "        text = u''\n",
      "        \n",
      "    return text\n",
      "\n",
      "#def compare_accuracy( lines, lines_expected ):\n",
      "#    return Levenshtein.distance( lines_to_comparable_text( lines ) , lines_to_comparable_text( lines_expected ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_anncestors( element ):\n",
      "    anncestors = [ element ];\n",
      "    anncestor = element.getparent()\n",
      "    \n",
      "    while anncestor != None :\n",
      "        #print 'loop'\n",
      "        anncestors.append( anncestor )\n",
      "        anncestor = anncestor.getparent()\n",
      "        \n",
      "    return anncestors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def text_from_lxml_object( obj):\n",
      "    if type(obj) is etree._ElementStringResult:\n",
      "        return u'' + obj\n",
      "    if type(obj) ==  etree._ElementUnicodeResult:\n",
      "        return u'' + obj \n",
      "    else:\n",
      "        try:\n",
      "            return etree.tostring( obj , method='text', encoding=\"UTF-8\") \n",
      "        except:\n",
      "            print type(obj)\n",
      "            print obj\n",
      "            \n",
      "            raise ''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from lxml import etree\n",
      "\n",
      "downloads_id =  582817308 \n",
      "download = get_download( downloads_id )\n",
      "raw_content = download[ 'raw_content' ]\n",
      "with open( '/tmp/' + str(downloads_id) , 'wb' ) as f:\n",
      "    f.write( raw_content )\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import tokenize\n",
      "\n",
      "def remove_duplicate_sentences( text_lines, story ):\n",
      "    comp_text = lines_to_comparable_text( text_lines )\n",
      "    sentences = [ sent.strip()  for sent in tokenize.sent_tokenize( comp_text ) ]\n",
      "    #print sentences\n",
      "    non_duplicate_sentences = [sentence for sentence in sentences if not sentence_is_duplicate( sentence, story ) ] \n",
      "    return u\"\\n\".join( non_duplicate_sentences )\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def text_children( element):\n",
      "    ret =  [ t for t in element.xpath(\"//text()\" ) if t.getparent() == element ]\n",
      "    assert len( ret ) <= 2\n",
      "    \n",
      "    if len( ret ) == 2:\n",
      "        assert ret[0].is_text\n",
      "        assert ret[1].is_tail\n",
      "    \n",
      "    for r in ret:\n",
      "        if r.is_text:\n",
      "            assert element.text == r\n",
      "        else:\n",
      "            assert r.is_tail\n",
      "            assert element.tail == r\n",
      "            \n",
      "    return ret"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def html_strip( str ):\n",
      "    if str.isspace() or str == '':\n",
      "        return u' '\n",
      "    \n",
      "    if str == '<':\n",
      "        return u' '        \n",
      "    \n",
      "    try:\n",
      "        return html.fromstring(str).text_content()    \n",
      "    except:\n",
      "        print \"Unexpected error on string '\" + str + \"'\" , sys.exc_info()[0]\n",
      "        #raise\n",
      "        return u''       \n",
      "\n",
      "def clean_for_comparison( str ):\n",
      "    if len(str) > 0:\n",
      "        ret = html_strip( str )\n",
      "    else:\n",
      "        return str\n",
      "    \n",
      "    return ret    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_with_mc_extractor( eto, method ):\n",
      "    story = eto['story']\n",
      "    preprocessed_lines = eto['preprocessed_lines']\n",
      "    title = story[u'title']\n",
      "    description = story[u'description']\n",
      "    \n",
      "    extract_result = extract_story( preprocessed_lines, title, description, 'HeuristicExtractor')\n",
      "    html_lines = get_extracted_text( extract_result )\n",
      "    \n",
      "    ret = {}\n",
      "    \n",
      "    ret['extracted_html'] = html_lines\n",
      "    \n",
      "    return ret\n",
      "\n",
      "def extract_with_heur( eto ):\n",
      "    return extract_with_mc_extractor( eto, 'HeuristicExtractor' )\n",
      "\n",
      "def extract_with_crf( eto ):\n",
      "    return extract_with_mc_extractor( eto, 'CrfExtractor' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import difflib\n",
      "from IPython.display import HTML\n",
      "\n",
      "from collections import Counter\n",
      "\n",
      "def ratcliff_obershelp_compare( actual_text, expected_text ):\n",
      "    \n",
      "    words_expected = actual_text.split()\n",
      "    words_crf      = expected_text.split()\n",
      "    \n",
      "    differ = difflib.Differ( )\n",
      "    \n",
      "    #print words_crf[:10]\n",
      "    #print words_expected[:10]\n",
      "    list( differ.compare( words_crf , words_expected ) )\n",
      "    counts = Counter([ d[0] for d in differ.compare( words_expected, words_crf   ) ])\n",
      "    \n",
      "    tp = counts[' ']\n",
      "    fp = counts['+']\n",
      "    fn = counts['-']\n",
      "    \n",
      "    if float(tp+fp) == 0:\n",
      "        precision = 0.0\n",
      "    else:\n",
      "        precision = tp/float(tp+fp)\n",
      "        \n",
      "    if float( tp + fn ) == 0:\n",
      "        recall = 0\n",
      "    else:\n",
      "        recall    = tp/float( tp + fn )\n",
      "    \n",
      "    if ( precision + recall ) > 0:\n",
      "        f1 = 2*(precision*recall)/( precision + recall )\n",
      "    else:\n",
      "        f1 = 0\n",
      "    \n",
      "    ret = { 'precision': precision,\n",
      "        'recall': recall,\n",
      "        'f1': f1\n",
      "    }\n",
      "    \n",
      "    return ret\n",
      "\n",
      "def compare_with_expected( extractor_name, actual_text, expected_text, story ):\n",
      "    #actual_text = lines_to_comparable_text( actual_lines )\n",
      "    #expected_text = lines_to_comparable_text( expected_lines )\n",
      "    ret = {}\n",
      "    ret[ extractor_name ] = ratcliff_obershelp_compare( actual_text, expected_text )\n",
      "    \n",
      "    #dedup_text = remove_duplicate_sentences( actual_lines, story )\n",
      "    \n",
      "    #ret[ extractor_name + \"_dedup\" ] = ratcliff_obershelp_compare( dedup_text, expected_text )\n",
      "    \n",
      "    return ret"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_extraction_results( eto ):\n",
      "\n",
      "    raw_content = eto[ 'raw_content' ]\n",
      "    \n",
      "    ret = {}\n",
      "    \n",
      "    ret['heur'] = extract_with_heur( eto )\n",
      "    ret['crf'] = extract_with_crf( eto )\n",
      "    ret['boiler_pipe'] =   extract_with_boilerpipe( raw_content)\n",
      "    ret['python_readibilty'] = { 'extracted_html': extract_with_python_readability( raw_content ) }\n",
      "    ret['py_goose']  = { 'extracted_html': extract_with_python_goose( raw_content ) }\n",
      "    ret['justext'] =  { 'extracted_html': extract_with_justext( raw_content ) }\n",
      "    \n",
      "    for method, result in ret.iteritems():\n",
      "        if 'extracted_text' not in result:\n",
      "            result['extracted_text'] = lines_to_comparable_text( result['extracted_html' ] )\n",
      "            \n",
      "    return ret"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compare_extractors_for_download( downloads_id ):\n",
      "    \n",
      "    eto = create_extractor_training_object( downloads_id )\n",
      "    \n",
      "    return comp_extractors( eto )    \n",
      "    \n",
      "def comp_extractors( eto ):    \n",
      "    downloads_id = eto['downloads_id']\n",
      "    media_id     = eto['media_id' ]\n",
      "    story = eto['story']\n",
      "    raw_content = eto['raw_content']\n",
      "    preprocessed_lines = eto['preprocessed_lines']\n",
      "    expected_text = eto['expected_text']\n",
      "        \n",
      "    title = story[u'title']\n",
      "    description = story[u'description']\n",
      "    url = story[u'url']\n",
      "    \n",
      "    extraction_results = get_extraction_results( eto )\n",
      "    \n",
      "    comp_results = {}\n",
      "        \n",
      "    comp_results['downloads_id'] = downloads_id\n",
      "    \n",
      "    comp_results['media_id']  = media_id\n",
      "    \n",
      "    for name, value in extraction_results.iteritems():\n",
      "        #print name, value\n",
      "        comp_results.update (compare_with_expected( name, value['extracted_text'], expected_text, story ) )\n",
      "    \n",
      "    return comp_results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_extractor_training_object( downloads_id, expected_text=None ):\n",
      "    download = get_download( downloads_id )\n",
      "    \n",
      "    raw_content = download[u'raw_content']\n",
      "    stories_id = download[u'stories_id']\n",
      "    \n",
      "    #print download['url']\n",
      "    \n",
      "    story = requests.get('https://api.mediacloud.org/api/v2/stories/single/'+str(stories_id)+'?key='+api_key)\n",
      "    \n",
      "    story = story.json()[0]\n",
      "    \n",
      "    story_lines = get_story_lines( raw_content )\n",
      "    #print story_lines.content\n",
      "    preprocessed_lines = story_lines.json()\n",
      "    \n",
      "    if not expected_text:\n",
      "        expected_lines = get_extractor_training_text( downloads_id, preprocessed_lines  )\n",
      "        expected_text  = lines_to_comparable_text( expected_lines )\n",
      "\n",
      "    ret = { 'downloads_id': downloads_id,\n",
      "           'raw_content': raw_content,\n",
      "           'media_id': story['media_id'],\n",
      "           'story': story,\n",
      "           'preprocessed_lines': preprocessed_lines,\n",
      "           'expected_text': expected_text\n",
      "           }\n",
      "    \n",
      "    return ret"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "\n",
      "sys.path.append('../')\n",
      "\n",
      "import mc_config\n",
      "\n",
      "def get_db_info():\n",
      "    config_file = mc_config.read_config()\n",
      "    \n",
      "    db_infos = config_file['database']\n",
      "    db_info = next (db_info for db_info in db_infos if db_info['port'] == '6000' )\n",
      "    return db_info\n",
      "\n",
      "import psycopg2\n",
      "#import solr_reimport\n",
      "import psycopg2.extras\n",
      "\n",
      "#db_info = get_db_info()\n",
      "\n",
      "#conn = psycopg2.connect( database=db_info['db'], user=db_info['user'], \n",
      "#                        password=db_info['pass'], host=db_info['host'], port=db_info['port'] )\n",
      "\n",
      "\n",
      "story_sentence_counts_cache = {}\n",
      "\n",
      "def get_sentence_counts( sentence, story ):\n",
      "\n",
      "    stories_id = story['stories_id']\n",
      "    \n",
      "    if not stories_id in story_sentence_counts_cache:\n",
      "        story_sentence_counts_cache[ stories_id ] = {}\n",
      "        \n",
      "    if sentence in story_sentence_counts_cache[ stories_id ]:\n",
      "        return story_sentence_counts_cache[stories_id ][sentence]\n",
      "        \n",
      "    cursor = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)\n",
      "    query = '''               \n",
      "                   SELECT MIN( story_sentence_counts_id) AS story_sentence_counts_id, sentence_count, first_stories_id,\n",
      "                   sentence_md5\n",
      "            FROM story_sentence_counts\n",
      "            WHERE sentence_md5  = md5(%(sentence)s)\n",
      "              AND media_id = %(media_id)s\n",
      "              AND publish_week =  DATE_TRUNC( 'week', %(publish_date)s::date )\n",
      "            GROUP BY story_sentence_counts_id\n",
      "    '''\n",
      "    \n",
      "    #print sentence\n",
      "    #md5_sum = md5.new( sentence ).hexdigest()\n",
      "    \n",
      "    params = { 'sentence': sentence,\n",
      "                            'media_id': story['media_id'], \n",
      "                            'publish_date': story['publish_date']\n",
      "                            } \n",
      "    \n",
      "    #print params\n",
      "    \n",
      "    #print eto[ 'story'] ['stories_id' ]\n",
      "    cursor.execute( query, params )\n",
      "    \n",
      "    fetched = cursor.fetchall()\n",
      "    \n",
      "    if len( fetched ) == 0:\n",
      "        story_sentence_counts_cache[ stories_id ][sentence] = None\n",
      "    else:\n",
      "        story_sentence_counts_cache[ stories_id ][sentence] = dict(fetched[0])\n",
      "        \n",
      "    return story_sentence_counts_cache[stories_id ][sentence]\n",
      "    \n",
      "def sentence_is_duplicate( sentence, story ):\n",
      "    sentence_counts = get_sentence_counts( sentence, story )\n",
      "    \n",
      "    if sentence_counts != None:\n",
      "        if sentence_counts['sentence_count'] > 1:\n",
      "            #print \"duplicate sentence\", sentence\n",
      "            return True\n",
      "        elif sentence_counts['first_stories_id'] == story['stories_id']:\n",
      "            return True\n",
      "            #print \"duplicate sentence (diff first_stories_id) \", sentence\n",
      "            \n",
      "    else:\n",
      "        return False\n",
      "        pass\n",
      "        #print \"sentence not found \", sentence \n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_extractor_training_objects_legacy ( downloads_ids ):\n",
      "    print downloads_ids\n",
      "    extractor_training_objects = []\n",
      "    for downloads_id in downloads_ids[:]:\n",
      "        print 'downloads_id:', downloads_id\n",
      "        extractor_training_objects.append( create_extractor_training_object( downloads_id ) )\n",
      "        \n",
      "    return extractor_training_objects"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sqlite3\n",
      "\n",
      "def get_extractor_training_objects_sqlite( db_file ):\n",
      "    \n",
      "    db = sqlite3.connect( db_file )\n",
      "    db.row_factory = sqlite3.Row\n",
      "    \n",
      "    cursor = db.cursor()\n",
      "    \n",
      "    cursor.execute( \"SELECT * from dlannotations  where selected_texts_json is not null order by downloads_id\" )\n",
      "    \n",
      "    extractor_training_objects = []\n",
      "    \n",
      "    skipped_downloads = 0\n",
      "    for row in list( cursor.fetchall() )[:]:\n",
      "        row =  dict([ (k, row[k]) for k in row.keys() ])\n",
      "        \n",
      "        #print row\n",
      "        \n",
      "        row['annotations'] = json.loads( row['annotations_json'] )\n",
      "        row['raw_content'] = u'' + row['raw_content']\n",
      "        row['selected_texts'] = json.loads( row['selected_texts_json'] )\n",
      "    \n",
      "        annotations = row['annotations']\n",
      "        download = get_download( row['downloads_id'] )\n",
      "    \n",
      "        assert row['selected_texts'] != None\n",
      "        assert row['selected_texts'] > 0\n",
      "        \n",
      "        eto = create_extractor_training_object( row['downloads_id'], expected_text=u\"\\n\".join(row['selected_texts']) )\n",
      "               \n",
      "        if eto['raw_content'] != row['raw_content']:\n",
      "            #TODO figure out why these may differ\n",
      "            pass\n",
      "        \n",
      "            #d = difflib.Differ()\n",
      "            #diff = d.compare(eto['raw_content'].splitlines(1), row['raw_content'].splitlines(1))\n",
      "            #print '\\n'.join(diff)\n",
      "        \n",
      "        extractor_training_objects.append( eto )\n",
      "    \n",
      "    \n",
      "    print \"skipped\", skipped_downloads\n",
      "    print \"processed\", len(extractor_training_objects)\n",
      "    \n",
      "    return extractor_training_objects"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "def get_data_frame_from_comparision_objects( comparison_objects ):\n",
      "    \n",
      "    new_comps = []\n",
      "    for comp in comparison_objects:\n",
      "        \n",
      "        new_comp = {}\n",
      "        new_comp = { 'downloads_id': comp['downloads_id'] }\n",
      "        \n",
      "        extractor_types = [ k for k in comp.keys() if k not in { 'downloads_id', 'media_id' }  ]\n",
      "        \n",
      "        for extractor_type in extractor_types:\n",
      "            new_comp.update([ ( k + '_' + extractor_type , v) for k,v in comp[ extractor_type ].iteritems() ])\n",
      "            \n",
      "        new_comps.append( new_comp )\n",
      "        \n",
      "    df = pd.DataFrame( new_comps )\n",
      "    df.set_index('downloads_id', inplace=True )\n",
      "    return df\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import boilerpipe.extract\n",
      "\n",
      "def extract_with_py_boilerpipe( raw_content ):\n",
      "    e = boilerpipe.extract.Extractor( extractor='ArticleExtractor', html=raw_content )\n",
      "    html = e.getHTML()\n",
      "\n",
      "    ret = { 'extracted_html': html }\n",
      "    return ret\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def print_results_by_measurement_type( df ):\n",
      "    df.describe(percentiles=[.5] )\n",
      "    result_types = [ 'precision', 'recall', 'f1' ]\n",
      "    for result_type in result_types:\n",
      "        res_columns = [ col for col in df.columns if col.startswith( result_type ) ]\n",
      "        #df.ix[:,['f1_boiler_pipe',\t'f1_crf',\t'f1_heur', 'f1_python_readibilty']].describe()\n",
      "        print df.ix[:,res_columns].describe( percentiles=[0.02, 0.05,.1,0.5])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Flags"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "regenerate_extractor_training_objects = True\n",
      "regenerate_media_id_media_map         = True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Constants"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "brazil_downloads_ids = [391881020,401370599,412896439,412952145,412977048,413024519,413657081,413835576,414040102, \n",
      "                        414257623,414377428,414480464,414818749,414983458,415185946,415186582,415197547,415424551,\n",
      "                        415978069,416026460,416026587,416047494,416047513,416210404,416263840,416306952,416426245,\n",
      "                        416655019,416730837,416802690,417347290,417347524,417368539,417389613,417477837,417653177,\n",
      "                        418489742,418544762,418574641,418648698,418661859,419404469,419440474,419483895,419873979,\n",
      "                        420430754,420599387,420666122,421520860,421834553,422181106,422280595,422910963,423318170,\n",
      "                        424080271,424369085,424796346,424840366,425206279,426405203,426560018,426632784,426709900,\n",
      "                        428449440,429607289,430363249,430995428,433457459,435624796,435659593,461175103,461175549,\n",
      "                        461176415,461176844,461177487,461178557,461178590,461179203,461179222,461179441,461179762,\n",
      "                        461179818,461179954,461179956,461180307,461181039,461181597,461186137,461186258,461186833,\n",
      "                        461187188,461187261,461187577,461188549,461189069,461190586,461193383]\n",
      "\n",
      "sqlite_db_file = 'extractor_train_dbs/dev_2014-11-06T10_18_57-0500.db'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Data Analysis"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Load Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "extractor_training_objects = []\n",
      "if regenerate_extractor_training_objects:\n",
      "    eto_brazil = get_extractor_training_objects_legacy( brazil_downloads_ids )\n",
      "    eto_sqlite = get_extractor_training_objects_sqlite( sqlite_db_file )\n",
      "    extractor_training_objects.extend( eto_brazil  )\n",
      "    extractor_training_objects.extend( eto_sqlite )\n",
      "    cPickle.dump( extractor_training_objects, open(\"extractor_traning_objects.pickle\", \"wb\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[391881020, 401370599, 412896439, 412952145, 412977048, 413024519, 413657081, 413835576, 414040102, 414257623, 414377428, 414480464, 414818749, 414983458, 415185946, 415186582, 415197547, 415424551, 415978069, 416026460, 416026587, 416047494, 416047513, 416210404, 416263840, 416306952, 416426245, 416655019, 416730837, 416802690, 417347290, 417347524, 417368539, 417389613, 417477837, 417653177, 418489742, 418544762, 418574641, 418648698, 418661859, 419404469, 419440474, 419483895, 419873979, 420430754, 420599387, 420666122, 421520860, 421834553, 422181106, 422280595, 422910963, 423318170, 424080271, 424369085, 424796346, 424840366, 425206279, 426405203, 426560018, 426632784, 426709900, 428449440, 429607289, 430363249, 430995428, 433457459, 435624796, 435659593, 461175103, 461175549, 461176415, 461176844, 461177487, 461178557, 461178590, 461179203, 461179222, 461179441, 461179762, 461179818, 461179954, 461179956, 461180307, 461181039, 461181597, 461186137, 461186258, 461186833, 461187188, 461187261, 461187577, 461188549, 461189069, 461190586, 461193383]\n",
        "downloads_id: 391881020\n"
       ]
      },
      {
       "ename": "ConnectionError",
       "evalue": "HTTPConnectionPool(host='0', port=3000): Max retries exceeded with url: /api/v2/extractlines/story_lines?key=f66a50230d54afaf18822808aed649f1d6ca72b08fb06d5efb6247afe9fbae52 (Caused by <class 'socket.error'>: [Errno 111] Connection refused)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-32-3474e0c31788>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mextractor_training_objects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mregenerate_extractor_training_objects\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0meto_brazil\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_extractor_training_objects_legacy\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mbrazil_downloads_ids\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0meto_sqlite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_extractor_training_objects_sqlite\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0msqlite_db_file\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mextractor_training_objects\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0meto_brazil\u001b[0m  \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-25-90252d0ef62c>\u001b[0m in \u001b[0;36mget_extractor_training_objects_legacy\u001b[1;34m(downloads_ids)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdownloads_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdownloads_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[1;34m'downloads_id:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownloads_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mextractor_training_objects\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mcreate_extractor_training_object\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdownloads_id\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mextractor_training_objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-23-493c97eed8c9>\u001b[0m in \u001b[0;36mcreate_extractor_training_object\u001b[1;34m(downloads_id, expected_text)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mstory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mstory_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_story_lines\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mraw_content\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;31m#print story_lines.content\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mpreprocessed_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstory_lines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-5-7f4917920037>\u001b[0m in \u001b[0;36mget_story_lines\u001b[1;34m(raw_content)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'Content-type'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'application/json'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     story_lines = requests.put('http://0:3000/api/v2/extractlines/story_lines',data=json.dumps(story_lines_params), \n\u001b[1;32m---> 21\u001b[1;33m                                params={ 'key': loc_key },headers=headers)\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mstory_lines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/requests/api.pyc\u001b[0m in \u001b[0;36mput\u001b[1;34m(url, data, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \"\"\"\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'put'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/requests/api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert)\u001b[0m\n\u001b[0;32m    381\u001b[0m             \u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         }\n\u001b[1;32m--> 383\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    484\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melapsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0m_ProxyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='0', port=3000): Max retries exceeded with url: /api/v2/extractlines/story_lines?key=f66a50230d54afaf18822808aed649f1d6ca72b08fb06d5efb6247afe9fbae52 (Caused by <class 'socket.error'>: [Errno 111] Connection refused)"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "extractor_training_objects = cPickle.load( open( \"extractor_traning_objects.pickle\", \"rb\") )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Look up Media Tags"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import itertools\n",
      "from collections import Counter\n",
      "\n",
      "mc = mediacloud.api.MediaCloud(api_key)\n",
      "\n",
      "if regenerate_media_id_media_map:\n",
      "    media_id_media_map = {}\n",
      "    \n",
      "    for media_id in list(media_ids)[:]:\n",
      "        media = mc.media( media_id )\n",
      "        media[ 'media_source_tags_ids' ] = set( [ media_source_tag['tags_id'] \n",
      "                                                 for media_source_tag in media['media_source_tags'] ] )\n",
      "        media_id_media_map[ media_id ] = media\n",
      "    \n",
      "    cPickle.dump( media_id_media_map, open(\"media_id_media_map.pickle\", \"wb\"))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "media_id_media_map = cPickle.load( open( \"media_id_media_map.pickle\", \"rb\") )    \n",
      "\n",
      "media_tag_counts = Counter(list ( itertools.chain.from_iterable( media_source['media_source_tags_ids'] for media_source in media_id_media_map.values() )) ) \n",
      "tags_id_to_media_tags_map = {}\n",
      "for media_tag in media_id_media_map.values():\n",
      "    source_tags = media_tag[ 'media_source_tags' ]\n",
      "    for source_tag in source_tags:\n",
      "        tags_id_to_media_tags_map[ source_tag[ 'tags_id' ] ] = source_tag"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[ (tags_id_to_media_tags_map[tag_id], count) for tag_id, count in media_tag_counts.most_common( 15 ) ]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Run extractors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import boilerpipe.extract\n",
      "\n",
      "e = boilerpipe.extract.Extractor( extractor='ArticleExtractor', html=raw_content )\n",
      "print e.getHTML()\n",
      "e."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw_content = eto['raw_content']\n",
      "with tempfile.NamedTemporaryFile( suffix='.html', delete=False ) as t:\n",
      "    #print t.name\n",
      "\n",
      "    UTF8Writer = codecs.getwriter('utf8')\n",
      "    t.file = UTF8Writer(t.file)\n",
      "    t.file.write( raw_content )\n",
      "\n",
      "    t.close()\n",
      "    #time.sleep( 2 )\n",
      "    #print \"original article tmp file \", t.name\n",
      "    \n",
      "    #input_file = '/tmp/416655019.htm'\n",
      "    input_file = t.name\n",
      "\n",
      "input_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#eto = extractor_training_objects[ 0 ]\n",
      "#eto.keys()\n",
      "#print eto['expected_text']\n",
      "#get_extraction_results( eto )\n",
      "#comp_extractors ( eto )\n",
      "\n",
      "extraction_results = []\n",
      "\n",
      "for eto in  extractor_training_objects[:2]:\n",
      "    er = dict( eto )\n",
      "    er[ 'extractor_results'] = get_extraction_results( eto )\n",
      "    \n",
      "    extraction_results.append( er )\n",
      "\n",
      "eto.keys()    \n",
      "#er.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "comps_downloads = []\n",
      "processed = 0\n",
      "skipped = 0\n",
      "for extractor_training_object in extractor_training_objects[:10]:\n",
      "    print 'processed ', processed\n",
      "    print 'skipped ', skipped\n",
      "    print extractor_training_object[ 'downloads_id']\n",
      "    try:\n",
      "        res = comp_extractors( extractor_training_object )\n",
      "        #print res\n",
      "        comps_downloads.append( res )\n",
      "        processed += 1\n",
      "    except Exception, e:\n",
      "        print \"error on download{}\".format( extractor_training_object[ 'downloads_id'] )\n",
      "        e = sys.exc_info()\n",
      "        \n",
      "        import traceback\n",
      "        \n",
      "        traceback.print_exc()\n",
      "        print e\n",
      "        raise e\n",
      "        skipped += 1\n",
      "\n",
      "e\n",
      "#extractor_training_objects\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def filter_by_media_tags_id( comps_downloads, media_tags_ids ):\n",
      "    media_ids_matching = set()\n",
      "    #print media_id_media_map\n",
      "    for media_id, media in media_id_media_map.iteritems():\n",
      "        if not media[ 'media_source_tags_ids'].isdisjoint( media_tags_ids ):\n",
      "            media_ids_matching.add( media_id )\n",
      "            \n",
      "    return  [cd for cd in comps_downloads if cd['media_id'] in media_ids_matching ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Results"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Results Overall"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = get_data_frame_from_comparision_objects( comps_downloads )\n",
      "print_results_by_measurement_type( df )\n",
      "#df.describe()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Results by Subset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "regional = { 2453107 }\n",
      "print \"region / pew knight sutdy / 245107 \"\n",
      "df = get_data_frame_from_comparision_objects( filter_by_media_tags_id( comps_downloads, regional ) )\n",
      "print_results_by_measurement_type( df )\n",
      "\n",
      "ap_english_us_top_25 = { 2453107 }\n",
      "print \"ap_english_us_top25 / 8875027 \"\n",
      "df = get_data_frame_from_comparision_objects( filter_by_media_tags_id( comps_downloads, regional ) )\n",
      "print_results_by_measurement_type( df )\n",
      "\n",
      "political_blogs = { 125 }\n",
      "print \"political blogs / 125\"\n",
      "df = get_data_frame_from_comparision_objects( filter_by_media_tags_id( comps_downloads, political_blogs ) )\n",
      "print_results_by_measurement_type( df )\n",
      "\n",
      "\n",
      "russian = { 7796878 }\n",
      "print 'russian'\n",
      "df = get_data_frame_from_comparision_objects( filter_by_media_tags_id( comps_downloads, russian ) )\n",
      "print_results_by_measurement_type( df )\n",
      "\n",
      "print 'brazil'\n",
      "df = get_data_frame_from_comparision_objects( filter_by_media_tags_id( comps_downloads, {8877968,  8877969, 8877973, 8877970 } ) )\n",
      "print_results_by_measurement_type( df )\n",
      "\n",
      "arabic = { 8878255 }\n",
      "print 'arabic'\n",
      "df = get_data_frame_from_comparision_objects( filter_by_media_tags_id( comps_downloads, arabic ) )\n",
      "print_results_by_measurement_type( df )\n",
      "                                             "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.keys()\n",
      "cols = [ column for column in df.columns if column.startswith('f1recall_' ) and not column.endswith('_dedup')]\n",
      "\n",
      "df.sort( ['f1_python_readibilty'], ascending=[1] ).loc[:, cols ].head( 10 )\n",
      "#comps_downloads[ 0] "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[ cd for cd in comps_downloads if cd[ 'downloads_id' ] == 590441662  ]\n",
      "\n",
      "eto = [ extractor_training_object for extractor_training_object in extractor_training_objects  if extractor_training_object['downloads_id' ] == 590441662 ][0]\n",
      "\n",
      "print eto[ 'story']['url']\n",
      "print \"The problem is that this page has 2 articles on it. Readability zeros in on the second one instead of the first one\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eto = [ extractor_training_object for extractor_training_object in extractor_training_objects  if extractor_training_object['downloads_id' ] == 420599387][0]\n",
      "\n",
      "print eto[ 'story']['url']\n",
      "#print eto.keys()\n",
      "print eto['expected_lines']\n",
      "extract_with_python_readability( eto['raw_content'] )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}