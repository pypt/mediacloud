---
name: MediaWords

### database settings. at least one database connection must be defined
database:
    - label : "LABEL"
      type  : "pg"
      host  : "localhost"
      db    : "mediacloud"
      user  : "mediaclouduser"
      pass  : "mediacloud"

    - label : "test"
      type  : "pg"
      host  : "localhost"
      db    : "mediacloud_test"
      user  : "mediaclouduser"
      pass  : "mediacloud"

### MongoDB connection settings for storing downloads in GridFS
#mongodb_gridfs:
    ### Production database
    #mediawords:
        #host      : "localhost" 
        #port      : "27017" 
        #database  : "mediacloud_downloads_gridfs"
    ### Testing database
    #test:
        #host      : "localhost" 
        #port      : "27017" 
        #database  : "mediacloud_downloads_gridfs_test"

### Amazon S3 connection settings
#amazon_s3:

    ### Authentication credentials
    #access_key_id       : "AKIAIOSFODNN7EXAMPLE"
    #secret_access_key   : "wJalrXUtnFEMI/K7MDENG/bPxRfiCYzEXAMPLEKEY"

    ### Production bucket
    #mediawords:
        #bucket_name             : "mediacloud"
        #downloads_folder_name   : "downloads"

    ### Testing bucket
    #test:
        #bucket_name             : "mediacloud_test"
        #downloads_folder_name   : "downloads_test"

### Gearman (p5-Gearman-JobScheduler) configuration
### (to enable Gearman, add at least one server)
#gearman:

    ### Gearman servers (required)
    #servers:
    #    - "127.0.0.1:4730"

    ### Worker log directory (optional; default is "Sys::Path->logdir . '/gjs/'")
    #worker_log_dir: "data/gearman_worker_logs/"

    ### Email notifications
    #notifications:

        ### Email addresses to send notifications to (optional; default is [])
        #emails:
        #    - "jdoe@cyber.law.harvard.edu"

        ### "From:" address (optional; default is "gjs_donotreply@example.com")
        #from_address: "gjs@mediacloud.org"

        ### Email subject prefix (optional; default is "[GJS]")
        #subject_prefix: "[GJS]"

#mail:
    # Email address to receive bug report emails; leave empty for no bug reports via email
    #bug_email : ""

    # "From:" email address that is being set in emails sent by Media Cloud
    #from_address: "noreply@mediacloud.org"

### everything below is optional.  the system should work out of the box without 
### touching any of these other than calais_key for tagging

#session:
    #expires: 3600

    ### directory where web app sessions are stored.  default to $homedir/tmp
    #storage: "~/tmp/mediawords-session"

## Uncomment and fill in to use Google Analytics
#google_analytics:
#      account: "<ACOUNT>"
#      domainname: "<DOMAIN>"	

mediawords:
    ### defaults to http://$hostname:$port/.
    #base_url: "http://your.mediacloud.server/and/path"

    ### parent directory of the content/ directory with all downloaded content is stored
    #data_dir: "<bindir>/../data"
    
    ### Uncomment to use the directory given by <PATH> to temporarily store CSV files generated in the public data dumps
    ### Note that since these are uncompressed files they can be very large.
    #data_dump_tmp_dir : <PATH>

    ### script directory
    #script_dir: "<bindir>/../script"

    ## media_sets_id of the media set to display on the Dashboard home page
    ##default_media_set: 1

    # Uncomment to use JNI mode on Inline::Java. This improves performance but breaks the test suite.
    #inline_java_jni: "yes"

    ### HTTP user agent and the email address of the owner of the bot
    user_agent: "mediawords bot (http://cyber.law.harvard.edu)"
    owner: "mediawords@cyber.law.harvard.edu"

    ### Uncomment one or more storage methods to store downloads to; default is just "localfile"
    ### which would write downloads to separate gzipped files in /data/.
    ### The path of the last download storage method listed below will be stored in the database.
    #download_storage_locations:
        ### store downloads to local files
        #- localfile
        ### store downloads to Tar archives
        #- tar
        ### store downloads to Amazon S3
        #- amazon_s3
        ## store downloads to a MongoDB GridFS database
        #- gridfs

    ### Uncomment both to force reading Tar and file downloads from GridFS
    ### (useful if you moved downloads from Tar archives into GridFS and don't
    ### want to / can't change 'downloads.path' on PostgreSQL just now)
    #read_tar_downloads_from_gridfs  : "no"
    #read_file_downloads_from_gridfs : "no"

    ### fetch content from another instance of MediaCloud?
    #fetch_remote_content          : "yes"
    #fetch_remote_content_user     : username
    #fetch_remote_content_password : password
    #fetch_remote_content_url      : "http://admin.mediacloud.org/admin/downloads/view/"

    #### Defaults to kmeans uncomment to use Cluto
    #clustering_engine: "cluto"
    #cluto_binary: "/usr/local/cluto/bin/vcluster"

    ### needed for calais tagging
    #calais_key: "CALAISKEY"

    ### needed for yahoo tagging
    #yahoo_key: "YAHOOKEY"

    ### default to just NYTTopics
    #default_tag_module: "NYTTopics Calais"

    #Uncomment to put the site into overload. This disables functionality so that the site can handle more traffic.
    #site_overload_mode: "yes"

    #Uncomment to disable the use of a precalculated version of the top_500_words. 
    #disable_json_top_500_words_cache: "yes"

    # Uncomment to allow sentences to cross block level html elments such a <p> or <div> 
    #disable_block_element_sentence_splitting: "yes"

    # Uncomment to cause feeds to be downloaded and stories but not processed for stories.
    # Generally the only reason you would want to do this is to run a backup crawler
    #do_not_process_feeds: 'yes'

    #controls the maximum time SQL queries can run for -- time is in ms
    #uncomment to enable a 10 minute timeout
    #db_statement_timeout: "600000"

    # Uncommit to speed up slow queries by setting the Postgresql work_mem parameter to this value
    # By default the initial Postgresql value of work_mem is used
    # large_work_mem: "3GB"

    # An experiment parameter to dump stack traces in error message even if not in debug mode
    # NOTE: may leak DB passwords and is not to be use in production
    #always_show_stack_traces: "yes"

    # reCAPTCHA public key (used to prevent brute-force in the password reset form)
    # The default value was set up for http://127.0.0.1 and is a global key (should work across all domains)
    #recaptcha_public_key: "6LfEVt0SAAAAAFwQI0pOZ1bTHgDTpQcMeQY6VLd_"

    # reCAPTCHA private key (used to prevent brute-force in the password reset form)
    # The default value was set up for http://127.0.0.1 and is a global key (should work across all domains)
    #recaptcha_private_key: "6LfEVt0SAAAAABmI-8IJmx4g93eNcSeyeCxvLMs2"

    #uncomment to make the public homepage the default start page    
    #default_home_page: "dashboard/view"
    # downloads id under which to strip all non-ascii characters
    #ascii_hack_downloads_id: 123456789
    
    # settings for mediawords_web_store.pl script that does in process parallel fetching
    # web_store_num_parallel: 10
    # web_store_timeout: 90
    # web_store_per_domain_timeout: 1
    
    # tablespace in which to create temporary tables -- defaults to the postgres default
    # temporary_table_tablespace: temporary_tablespace
    
    # number of times to run each controversy_dump_time_slice model in MediaWords::CM::Model
    # controversy_model_reps: 25
    
    # url for solr word counting url
    # solr_wc_url: http://localhost:8080/wc
    
    # url for solr queries
    # solr_select_url: http://localhost:8983/solr/collection1/select
