---
name: MediaWords

### database settings. at least one database connection must be defined. the
### main "production" database should be the first one below.
database:

    # production
    - label : "LABEL"
      type  : "pg"
      host  : "localhost"
      port  : 5432
      db    : "mediacloud"
      user  : "mediaclouduser"
      pass  : "mediacloud"

    # unit tests
    - label : "test"
      type  : "pg"
      host  : "localhost"
      port  : 5432
      db    : "mediacloud_test"
      user  : "mediaclouduser"
      pass  : "mediacloud"

    # persistent "gearmand" queue
    - label : "gearman"
      type  : "pg"
      host  : "localhost"
      port  : 5432
      db    : "mediacloud_gearman"
      user  : "mediaclouduser"
      pass  : "mediacloud"


### MongoDB connection settings
#mongodb_gridfs:
    #host : "localhost"
    #port : "27017"
    #username : "mediaclouduser"    # optional
    #password : "mediacloud"        # optional
    ### Database for storing raw downloads
    #downloads:
        #database_name : "mediacloud_downloads_gridfs"
    ### Database for testing
    #test:
        #database_name : "mediacloud_downloads_gridfs_test"
    ### (optional) Database for storing CoreNLP annotator results
    #corenlp:
        #database_name : "mediacloud_corenlp"
    ### (optional) Database for storing Bit.ly API results
    #bitly:
        #database_name : "mediacloud_bitly"

### Amazon S3 connection settings
#amazon_s3:

    ### Authentication credentials
    #access_key_id       : "AKIAIOSFODNN7EXAMPLE"
    #secret_access_key   : "wJalrXUtnFEMI/K7MDENG/bPxRfiCYzEXAMPLEKEY"

    ### Bucket for storing downloads
    #downloads:
        #bucket_name    : "mediacloud"
        #directory_name : "downloads"

    ### Bucket for testing
    #test:
        #bucket_name    : "mediacloud_test"
        #directory_name : "downloads_test"

### Gearman worker (p5-Gearman-JobScheduler) configuration
### (to enable Gearman, add at least one server)
#gearman:

    ### Gearman servers (required)
    #servers:
        ### By default "gearmand" works on port 4730; "gearmand" run as a
        ### Supervisord daemon (see below) runs on 4731.
        ### It is recommended to use Media Cloud-provided Gearman instance
        ### instead of relying on system's because of a persistent
        ### (PostgreSQL-backed) job queue and Supervisord logging facilities.
        ### Default is a single server at "127.0.0.1:4731".
        #- "127.0.0.1:4731"

    ### Worker log directory (default is "data/gearman_worker_logs/"; if not
    ### set, will fallback to Sys::Path->logdir . '/gjs/')
    #worker_log_dir: "data/gearman_worker_logs/"

    ### Email notifications
    #notifications:

        ### Email addresses to send notifications to (optional; default is [])
        #emails:
        #    - "jdoe@cyber.law.harvard.edu"

        ### "From:" address (default is "noreply@mediacloud.org"; if not set,
        ### will fallback to "gjs_donotreply@example.com")
        #from_address: "gjs@mediacloud.org"

        ### Email subject prefix (default is "[GJS]"; if not set, will fallback
        ### to an empty string)
        #subject_prefix: "[GJS]"


### Gearman server (gearmand) configuration
#gearmand:

    ### To disable your very own "gearmand" instance managed by Supervisord,
    ### set the below to "no". Default is "yes".
    #enabled: "yes"

    ### Host to listen to. You can set the above parameter to an empty string
    ### so that Gearman will accept connections from anywhere; however, it is
    ### highly advised use secure channels (e.g. a SSH tunnel) to make Gearman
    ### accessible to "outside" instead. Default is "127.0.0.1".
    #listen: "127.0.0.1"

    ### Port to use in gearmand. Default port for vendor-provided Gearman
    ### deployments is 4730, but Media Cloud runs its own Gearman instance via
    ### Supervisord. Default is 4731.
    #port: 4731

### Supervisor (supervisord) configuration
#supervisor:

    ### The log directory for child process logs (absolute or relative to Media
    ### Cloud's root; must already exist)
    #childlogdir: "data/supervisor_logs/"

    # configure supervisor settings for mediacloud background daemons here.
    # the defaults should work for a small dev setup, but you will want to increase
    # numprocs for some daemons depending on load.  you can also set some daemons
    # not to autostart -- for instance you might want to change crawler.autostart
    # to 'false' to prevent the crawler from starting automatically on a dev machine.
    #programs:

        #crawler:
            #numprocs: 1
            #autostart: 'true'
            #autorestart: 'false'

        #extract_and_vector:
            #numprocs: 1
            #autostart: 'true'
            #autorestart: 'true'

        # other configurable supervisor programs
        #crf_web_service
        #gearmand
        #add_default_feeds
        #cm_mine_controversy
        #cm_dump_controversy
        #solr
        #solr_facets_wrapper_server
        #annotate_with_corenlp
        #bitly_enqueue_controversy_stories
        #bitly_fetch_story_stats
        #bitly_aggregate_story_stats


### CoreNLP annotator
#corenlp:

    ### Enable CoreNLP processing
    ### If enabled, CoreNLP processing will happen after every "content"
    ### download extraction
    #enabled: "no"

    ### Annotator URL
    #annotator_url: "http://www.example.com:8080/corenlp/annotator"

    ### Annotator timeout (in seconds)
    ### If you annotate huge chunks of text or the CoreNLP annotator is busy in
    ### general, you might want to increase this value even more because
    ### otherwise the Gearman worker will exit() often.
    #annotator_timeout: 600

    ### CoreNLP annotator "level"; you might want to use this configuration
    ### parameter to limit the scope of annotations returned from the service.
    ### Default is an empty string; you might want to set it to "ner".
    #annotator_level: ""

    ###
    ### Also see: "mongodb_gridfs/corenlp/database_name" configuration property above
    ###

### CRF model runner web service
#crf_web_service:

    ### To enable the CRF model runner via the web service (both the Java
    ### server and extractor client), set the below to "yes".
    ###
    ### If the service is enabled, Supervisord will start a CRF model runner
    ### web service which will then be used by the extractor.
    ###
    ### If the service is disabled, Supervisord will *not* start a CRF model
    ### runner web service, and extractor will use Inline::Java to run the CRF
    ### model instead.
    #enabled: "no"

    ### URL of the CRF model runner web service the extractor should use to do
    ### the CRF processing.
    ###
    ### If you are running the CRF model runner web service on the very same
    ### machine as the extractor, leave this parameter intact. The parameter is
    ### only useful if you are running the CRF model runner web service on a
    ### different machine than the extractor.
    #server_url: "http://127.0.0.1:8441/crf"

    ### Host(:port) the CRF model runner web service should to listen to for
    ### processing requests from the extractor.
    ###
    ### You can set the above parameter to an empty string so that CRF model
    ### runner web service will accept connections from anywhere. However, it
    ### is highly advised use secure channels (e.g. a SSH tunnel) to make CRF
    ### model runner accessible to "outside" instead.
    ###
    ### Default port is 8441.
    #listen: "127.0.0.1:8441"

    ### Number of threads the CRF model runner web service should spawn.
    ###
    ### More threads will serve more model runner requests per seconds, but
    ### will take up more memory.
    #number_of_threads: 32

### Bit.ly API
#bitly:

    ### Enable Bit.ly processing
    #enabled: "no"

    ### (Generic) Access Token
    ### Get one at: https://bitly.com/a/oauth_apps
    #access_token: ""

    ### API request timeout (in seconds)
    #timeout: 60

### Facebook API
### (see doc/README.facebook_api.markdown)
#facebook:

    ### Enable Facebook processing
    #enabled: "yes"

    ## App ID
    #app_id: ""

    ## App Secret
    #app_secret: ""

    ## Request timeout
    #timeout: 60

#mail:
    # Email address to receive bug report emails; leave empty for no bug reports via email
    #bug_email : ""

    # "From:" email address that is being set in emails sent by Media Cloud
    #from_address: "noreply@mediacloud.org"

### everything below is optional.  the system should work out of the box without
### touching any of these other than calais_key for tagging

#session:
    #expires: 3600

    ### directory where web app sessions are stored.  default to $homedir/tmp
    #storage: "~/tmp/mediawords-session"

## Uncomment and fill in to use Google Analytics
#google_analytics:
#      account: "<ACOUNT>"
#      domainname: "<DOMAIN>"

mediawords:
    #extractor_method: PythonReadability
    ### defaults to http://$hostname:$port/.
    #base_url: "http://your.mediacloud.server/and/path"

    ### Directory in which various kinds of data (logs, etc.) is being stored
    #data_dir: "<bindir>/../data"

    ### Uncomment to use the directory given by <PATH> to temporarily store CSV files generated in the public data dumps
    ### Note that since these are uncompressed files they can be very large.
    #data_dump_tmp_dir : <PATH>

    ### script directory
    #script_dir: "<bindir>/../script"

    ## media_sets_id of the media set to display on the Dashboard home page
    ##default_media_set: 1

    # Uncomment to use JNI mode on Inline::Java. This improves performance but breaks the test suite.
    #inline_java_jni: "yes"

    ### HTTP user agent and the email address of the owner of the bot
    user_agent: "mediawords bot (http://cyber.law.harvard.edu)"
    owner: "mediawords@cyber.law.harvard.edu"

    ### Uncomment one or more storage methods to store downloads in.
    ### Default is "postgresql" which stores downloads directly in the
    ### PostgreSQL database.
    ###
    ### Very short downloads will be stored directly in the database, under
    ### "downloads.path"
    ###
    ### The path of the last download storage method listed below will be
    ### stored in "downloads.path" database column.
    #download_storage_locations:
        ### store downloads in the PostgreSQL database, "raw_downloads" table
        #- postgresql
        ### store downloads in Amazon S3
        #- amazon_s3
        ### store downloads in MongoDB GridFS database
        #- gridfs

    ### Uncomment to force reading GridFS downloads from Amazon S3
    ### (useful if you're experiencing MongoDB downtime for some reason or are
    ### in the process of restoring downloads from S3 back to GridFS)
    #read_gridfs_downloads_from_s3 : "no"

    #### Defaults to kmeans uncomment to use Cluto
    #clustering_engine: "cluto"
    #cluto_binary: "/usr/local/cluto/bin/vcluster"

    ### needed for calais tagging
    #calais_key: "CALAISKEY"

    ### needed for yahoo tagging
    #yahoo_key: "YAHOOKEY"

    ### default to just NYTTopics
    #default_tag_module: "NYTTopics Calais"

    #Uncomment to put the site into overload. This disables functionality so that the site can handle more traffic.
    #site_overload_mode: "yes"

    #Uncomment to disable the use of a precalculated version of the top_500_words.
    #disable_json_top_500_words_cache: "yes"

    # Uncomment to cause feeds to be downloaded and stories but not processed for stories.
    # Generally the only reason you would want to do this is to run a backup crawler
    #do_not_process_feeds: 'yes'

    #controls the maximum time SQL queries can run for -- time is in ms
    #uncomment to enable a 10 minute timeout
    #db_statement_timeout: "600000"

    # Uncommit to speed up slow queries by setting the Postgresql work_mem parameter to this value
    # By default the initial Postgresql value of work_mem is used
    # large_work_mem: "3GB"

    # An experiment parameter to dump stack traces in error message even if not in debug mode
    # NOTE: may leak DB passwords and is not to be use in production
    #always_show_stack_traces: "yes"

    # reCAPTCHA public key (used to prevent brute-force in the password reset form)
    # The default value was set up for http://127.0.0.1 and is a global key (should work across all domains)
    #recaptcha_public_key: "6LfEVt0SAAAAAFwQI0pOZ1bTHgDTpQcMeQY6VLd_"

    # reCAPTCHA private key (used to prevent brute-force in the password reset form)
    # The default value was set up for http://127.0.0.1 and is a global key (should work across all domains)
    #recaptcha_private_key: "6LfEVt0SAAAAABmI-8IJmx4g93eNcSeyeCxvLMs2"

    #uncomment to make the public homepage the default start page
    #default_home_page: "dashboard/view"
    # downloads id under which to strip all non-ascii characters
    #ascii_hack_downloads_id: 123456789

    # settings for mediawords_web_store.pl script that does in process parallel fetching
    # web_store_num_parallel: 10
    # web_store_timeout: 90
    # web_store_per_domain_timeout: 1

    # tablespace in which to create temporary tables -- defaults to the postgres default
    # temporary_table_tablespace: temporary_tablespace

    # number of times to run each controversy_dump_time_slice model in MediaWords::CM::Model
    # controversy_model_reps: 25

    # url for solr word counting url.  if this is set, fetch word counts from a remote server
    # using this url; otherwise, generate word counts locally
    # solr_wc_url: http://localhost/api/v2/wc

    # mc api key for appending to sol_wc_url for fetching remote word counts
    # solr_wc_key: FOO

    # urls for solr queries, include multiple to make mc choose a random url from
    # the list for each solr query
    # solr_url:
    #    - http://localhost:8983/solr
    #    - http://127.0.0.1:8983/solr

    # By default API requests require a invalid 'key' param.
    # comment to allow api requests without a key.
    # WARNING DO NOT enable on a production or Internet connected instance
    # allow_unauthenticated_api_requests: yes

    # set to "yes" to skip requirement to run on the correct database schema version
    # ignore_schema_version: "no"

    # increment wc_cache_version to invalidate existing cache
    # wc_cache_version: 1

    # max number of iterations by the the spider in MediaWords::CM::Mine
    # mc_spider_iterations: 15

    # if set to 1, do all extractions in process rather than queueing to gearman
    # extract_in_process: 0
